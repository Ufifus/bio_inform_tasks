{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W2Vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3wCYiJ4t3Sui6YUYS4j3f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ufifus/bio_inform_tasks/blob/main/W2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio\n",
        "!pip install sentence_splitter"
      ],
      "metadata": {
        "id": "lDdBYkLhVy-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez, Medline\n",
        "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
        "import io, re, string, tqdm\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords as sw\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "C4-Yi33ZhLgC"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lKGcff3aQIoZ"
      },
      "outputs": [],
      "source": [
        "class Embedding_task:\n",
        "    def __init__(self, path_to_text, text, stopwords=None):\n",
        "        self.stopwords = stopwords\n",
        "        self.path_to_text = path_to_text\n",
        "        self.text = text\n",
        "\n",
        "    def clear_str(self, string):\n",
        "        replace_simbols = ['\"', \"'\", \"/\", \"\\\\\", \"[\", \"]\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\",]\n",
        "        for replace_simbol in replace_simbols:\n",
        "            string = string.replace(replace_simbol, ' ')\n",
        "            if self.stopwords is None:\n",
        "                pass\n",
        "            else:\n",
        "                for stopword in self.stopwords:\n",
        "                    string = string.replace(stopword, ' ')\n",
        "            return string\n",
        "\n",
        "    @classmethod\n",
        "    def get_stopwords(cls):\n",
        "        stopwords = sw.words('english')\n",
        "        return cls(stopwords=stopwords)\n",
        "\n",
        "    @classmethod\n",
        "    def create_file(cls, path_to_text):\n",
        "        text = io.open(path_to_text, 'w', encoding='utf-8')\n",
        "        return cls(path_to_text=path_to_text, text=text)\n",
        "\n",
        "    def record_text(self, rules, search_strings):\n",
        "        Entrez.email = rules\n",
        "        i = 0\n",
        "        splitter = SentenceSplitter(language='en')\n",
        "\n",
        "        for search_str in search_strings:\n",
        "            handle = Entrez.esearch(db='pubmed', sort='relevance', term=search_str, retmax='10000')\n",
        "            record = Entrez.read(handle)\n",
        "            rec_count = record['Count']\n",
        "            print(search_str, ' - ', rec_count)\n",
        "\n",
        "            idlist = record['IdList']\n",
        "            handle = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode=\"text\")\n",
        "            records = Medline.parse(handle)\n",
        "\n",
        "            for record in records:\n",
        "                try:\n",
        "                    title = self.clear_str(str(record['TI']))\n",
        "                except:\n",
        "                    title = ' . '\n",
        "\n",
        "                try:\n",
        "                    abstract = self.clear_str(str(record['AB']))\n",
        "                except:\n",
        "                    abstract = ' . '\n",
        "\n",
        "                ap = title + abstract\n",
        "                lines = splitter.split(ap)\n",
        "                for line in lines:\n",
        "                    self.text.write(line + '\\n')\n",
        "                i += 1\n",
        "                if i % 500 == 0:\n",
        "                    print(i, ap)\n",
        "\n",
        "        print(i, ap)\n",
        "        self.text.close()\n",
        "        return self\n",
        "\n",
        "    def get_attributes(self):\n",
        "      length_max = 0\n",
        "      with io.open(self.path_to_text, 'r', encoding='utf-8') as txt:\n",
        "          lines = txt.read().splitlines()\n",
        "          for line in lines:\n",
        "              if length_max < len(line.split()):\n",
        "                  length_max = len(line.split())\n",
        "      attributes = {\n",
        "          'num_lines': len(lines),\n",
        "          'length_max_line': length_max\n",
        "      }\n",
        "      return print('num_lines:', len(lines), 'length_max_line:', length_max)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vecrorization:\n",
        "    def __init__(self, path_to_text, vocab_size, max_len_seq,\n",
        "                 batch_size, train_data=None, vocab=None, vec_layer=None):\n",
        "      self.vocab = vocab\n",
        "      self.vocab_size = vocab_size\n",
        "      self.max_len_seq = max_len_seq\n",
        "      self.path_to_text = path_to_text\n",
        "      self.batch_size = batch_size\n",
        "      self.train_data = train_data\n",
        "\n",
        "    def prepocessing(self, input_text):\n",
        "      lowercase = tf.strings.lower(input_text)\n",
        "      return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "    def create_vocab(self):\n",
        "      vec_layer = tf.keras.layers.TextVectorization(\n",
        "          standardize=self.prepocessing,\n",
        "          max_tokens=self.vocab_size,\n",
        "          output_mode='int',\n",
        "          output_sequence_length=self.max_len_seq\n",
        "      )\n",
        "      vec_layer.adapt(self.text_ds().batch(self.batch_size))\n",
        "      return vec_layer\n",
        "\n",
        "    def text_ds(self):\n",
        "      return tf.data.TextLineDataset(self.path_to_text)\\\n",
        "                                      .filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "\n",
        "    def get_vocab(self):\n",
        "      self.vocab = self.create_vocab().get_vocabulary()\n",
        "      return self.vocab\n",
        "\n",
        "    def vectorize_text(self):\n",
        "      text_vector_ds = self.text_ds().batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\\\n",
        "                                                        .map(self.create_vocab()).unbatch()\n",
        "      sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "      return sequences\n",
        "\n",
        "    def generate_training_data(self, window_size, num_ns, seed):\n",
        "      targets, contexts, labels = [], [], []\n",
        "      sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(self.vocab_size)\n",
        "      for sequence in tqdm.tqdm(self.vectorize_text()):\n",
        "            positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "                sequence,\n",
        "                vocabulary_size=self.vocab_size,\n",
        "                sampling_table=sampling_table,\n",
        "                window_size=window_size,\n",
        "                negative_samples=0)\n",
        "\n",
        "            for target_word, context_word in positive_skip_grams:\n",
        "                context_class = tf.expand_dims(\n",
        "                    tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "                negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "                    true_classes=context_class,\n",
        "                    num_true=1,\n",
        "                    num_sampled=num_ns,\n",
        "                    unique=True,\n",
        "                    range_max=self.vocab_size,\n",
        "                    seed=seed,\n",
        "                    name=\"negative_sampling\")\n",
        "\n",
        "                negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "\n",
        "                context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "                label = tf.constant([1] + [0] * num_ns, dtype=\"int64\")\n",
        "\n",
        "                targets.append(target_word)\n",
        "                contexts.append(context)\n",
        "                labels.append(label)\n",
        "\n",
        "      self.train_data = {\n",
        "          'targets': np.array(targets),\n",
        "          'contexts': np.array(contexts)[:, :, 0],\n",
        "          'labels': np.array(labels)\n",
        "        }\n",
        "      return self\n",
        "\n",
        "    def create_dataset(self, batch_size, buffer_size):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(((self.train_data['targets'],\n",
        "                                                      self.train_data['contexts']),\n",
        "                                                        self.train_data['labels']))\n",
        "        dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "        print(dataset)\n",
        "        dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "2lUzUo01hQum"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                             embedding_dim,\n",
        "                                             input_length=1,\n",
        "                                             name='w2v_embedding')\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                              embedding_dim,\n",
        "                                              input_length=4)\n",
        "    \n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "    return dots"
      ],
      "metadata": {
        "id": "48qlSY_YHK-f"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_strings = [\"nafld and 2020/01/01:2021/10/26[dp]\"]\n",
        "path_to_text = 'w2v_text.txt'\n",
        "\n",
        "text = Embedding_task.create_file(path_to_text)\n",
        "text.record_text(\"e.p@d_health.pro\", search_strings)\n",
        "text.get_attributes()\n",
        "\n",
        "e = Vecrorization('w2v_text.txt', 10000, 50, 1024)\n",
        "e.generate_training_data(5, 3, 42)\n",
        "dataset = e.create_dataset(2048, 15000)"
      ],
      "metadata": {
        "id": "6L9mbfxfG4DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "vocab_size = e.vocab_size\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "luHQlN1NZ31U"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyBmtdK5Z9Gw",
        "outputId": "905d69a6-a7c7-4056-fd01-20c963c97934"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1131/1131 [==============================] - 94s 82ms/step - loss: 1.2104 - accuracy: 0.4496\n",
            "Epoch 2/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.9858 - accuracy: 0.5811\n",
            "Epoch 3/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.8802 - accuracy: 0.6349\n",
            "Epoch 4/20\n",
            "1131/1131 [==============================] - 85s 75ms/step - loss: 0.8002 - accuracy: 0.6753\n",
            "Epoch 5/20\n",
            "1131/1131 [==============================] - 85s 76ms/step - loss: 0.7308 - accuracy: 0.7104\n",
            "Epoch 6/20\n",
            "1131/1131 [==============================] - 85s 76ms/step - loss: 0.6692 - accuracy: 0.7408\n",
            "Epoch 7/20\n",
            "1131/1131 [==============================] - 85s 76ms/step - loss: 0.6150 - accuracy: 0.7663\n",
            "Epoch 8/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.5680 - accuracy: 0.7873\n",
            "Epoch 9/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.5278 - accuracy: 0.8038\n",
            "Epoch 10/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.4939 - accuracy: 0.8167\n",
            "Epoch 11/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.4653 - accuracy: 0.8266\n",
            "Epoch 12/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.4414 - accuracy: 0.8342\n",
            "Epoch 13/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.4214 - accuracy: 0.8400\n",
            "Epoch 14/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.4046 - accuracy: 0.8447\n",
            "Epoch 15/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.3905 - accuracy: 0.8483\n",
            "Epoch 16/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.3785 - accuracy: 0.8512\n",
            "Epoch 17/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.3684 - accuracy: 0.8535\n",
            "Epoch 18/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.3598 - accuracy: 0.8553\n",
            "Epoch 19/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.3525 - accuracy: 0.8568\n",
            "Epoch 20/20\n",
            "1131/1131 [==============================] - 86s 76ms/step - loss: 0.3461 - accuracy: 0.8580\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe118029290>"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fixe9I5Mb6lY",
        "outputId": "9189d7e1-60be-4a18-ad4d-3a417091c6c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "\n",
        "\n",
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jbc2jzOwb9Z7",
        "outputId": "11fee047-312f-4194-8722-bf23a7d44bf3"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1691cc7a-270a-441b-8c65-0f4ee07e8583\", \"vectors.tsv\", 34285004)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_049ad14b-d369-42c0-aa63-24abe1229f12\", \"metadata.tsv\", 84176)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[-20::])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ff9LndCkHlf",
        "outputId": "30352bcf-7f88-4350-ef94-9dae9d50411f"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nmidb', 'nmethyltransferase', 'nma', 'nitrite', 'ni', 'nglycosylation', 'nglycopeptides', 'neuromuscular', 'neurocognitive', 'nephrolithiasis', 'ndb', 'nchcc', 'nb', 'nagala', 'musclederived', 'multisystemic', 'multidrug', 'mrtfa', 'mrjps', 'mrilsn']\n"
          ]
        }
      ]
    }
  ]
}